{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持存储稠密向量并且基于稠密向量进行距离计算和近似检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"dense_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    collection.insert([list(range(100)), vectors.tolist()])\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 5. Verify Query\n",
    "    logging.info(\"Running query...\")\n",
    "    res = collection.query(expr=\"id == 0\", output_fields=[\"vector\"])\n",
    "    logging.info(\"Query results:\")\n",
    "    logging.info(f\"Query returned {len(res)} results:\")\n",
    "    for idx, result in enumerate(res):\n",
    "        logging.info(f\"Result {idx}: ID={result['id']}, Vector={result['vector'][:5]}...\")  # Show first 5 elements\n",
    "\n",
    "    # 6. Verify Search\n",
    "    logging.info(\"Running search...\")\n",
    "    search_result = collection.search(\n",
    "        vectors[:1].tolist(),\n",
    "        \"vector\",\n",
    "        {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n",
    "        limit=3,\n",
    "        output_fields=[\"vector\"]\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Search results:\")\n",
    "    logging.info(f\"Search returned {len(search_result[0])} results:\")\n",
    "    for idx, hit in enumerate(search_result[0]):\n",
    "        logging.info(f\"Rank {idx+1}: ID={hit.id}, Distance={hit.distance:.4f}\")\n",
    "        logging.info(f\"Vector: {hit.entity.fields['vector'][:5]}...\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"dense_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持存储稀疏向量并且基于稀疏向量进行距离计算和近似检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "from scipy.sparse import random as sparse_random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    utility.drop_collection(\"sparse_test\")\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    metric_type = \"IP\"  \n",
    "    collection = Collection(\n",
    "        \"sparse_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.SPARSE_FLOAT_VECTOR)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"SPARSE_INVERTED_INDEX\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16, \"inverted_index_algo\": \"DAAT_MAXSCORE\"}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    sparse_vectors = [\n",
    "        {\"id\": 0, \"vector\": {1: 0.5, 100: 0.3, 500: 0.8}},\n",
    "        {\"id\": 1, \"vector\": {10: 0.1, 200: 0.7, 500: 0.9}},\n",
    "    ]\n",
    "    \n",
    "    collection.insert(sparse_vectors)\n",
    "    logging.info(f\"Inserted {len(sparse_vectors)} vectors\")\n",
    "\n",
    "    # 5. Verify Query\n",
    "    logging.info(\"Running query...\")\n",
    "    res = collection.query(expr=\"id == 0\", output_fields=[\"vector\"])\n",
    "    logging.info(\"Query results:\")\n",
    "    logging.info(f\"Query returned {len(res)} results:\")\n",
    "    for idx, result in enumerate(res):\n",
    "        logging.info(f\"Result {idx}: ID={result['id']}, Vector={result['vector']}\") \n",
    "\n",
    "    # 6. Verify Search\n",
    "    logging.info(\"Running search...\")\n",
    "    search_result = collection.search(\n",
    "        [sparse_vectors[1][\"vector\"]],\n",
    "        \"vector\",\n",
    "        {\"metric_type\": metric_type, \"params\": {\"drop_ratio_search\": 0.2}},\n",
    "        limit=3,\n",
    "        output_fields=[\"vector\"]\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Search results:\")\n",
    "    logging.info(f\"Search returned {len(search_result[0])} results:\")\n",
    "    for idx, hit in enumerate(search_result[0]):\n",
    "       logging.info(f\"Result {idx}: ID={result['id']}, Vector={result['vector']}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"sparse_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持2~4096维度的稠密向量；是否支持2~4096维度的索引创建和查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "from scipy.sparse import random as sparse_random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "    \n",
    "    \n",
    "    # Test multiple dimensions\n",
    "    dimensions = [2, 64, 256, 1024, 4096]\n",
    "    metric_type = \"L2\"\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        logging.info(f\"{'='*30} Testing Dimension: {dim} {'='*30}\")\n",
    "\n",
    "        # Create dense vector collection\n",
    "        logging.info(\"Creating collection...\")\n",
    "        dim = 768\n",
    "        metric_type = \"L2\"  # or \"IP\"\n",
    "        collection = Collection(\n",
    "            \"dim_test\",\n",
    "            CollectionSchema([\n",
    "                FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "                FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "            ]),\n",
    "            consistency_level=\"Strong\" \n",
    "        )\n",
    "        logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "        # Create index and load\n",
    "        logging.info(\"Creating index...\")\n",
    "        collection.create_index(\n",
    "            \"vector\",\n",
    "            {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "        )\n",
    "        collection.load()\n",
    "        logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "        # Insert dense vectors\n",
    "        logging.info(\"Generating and inserting vectors...\")\n",
    "        vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "        collection.insert([list(range(100)), vectors.tolist()])\n",
    "        logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "        # Verify Query\n",
    "        logging.info(\"Running query...\")\n",
    "        res = collection.query(expr=\"id == 0\", output_fields=[\"vector\"])\n",
    "        logging.info(\"Query results:\")\n",
    "        logging.info(f\"Query returned {len(res)} results:\")\n",
    "        for idx, result in enumerate(res):\n",
    "            logging.info(f\"Result {idx}: ID={result['id']}, Vector={result['vector'][:5]}...\")  # Show first 5 elements\n",
    "\n",
    "        # Verify Search\n",
    "        logging.info(\"Running search...\")\n",
    "        search_result = collection.search(\n",
    "            vectors[:1].tolist(),\n",
    "            \"vector\",\n",
    "            {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n",
    "            limit=3,\n",
    "            output_fields=[\"vector\"]\n",
    "        )\n",
    "        \n",
    "        logging.info(\"Search results:\")\n",
    "        logging.info(f\"Search returned {len(search_result[0])} results:\")\n",
    "        for idx, hit in enumerate(search_result[0]):\n",
    "            logging.info(f\"Rank {idx+1}: ID={hit.id}, Distance={hit.distance:.4f}\")\n",
    "            logging.info(f\"Vector: {hit.entity.fields['vector'][:5]}...\")\n",
    "\n",
    "        # Cleanup\n",
    "        logging.info(\"Cleaning up...\")\n",
    "        utility.drop_collection(\"dim_test\")\n",
    "        logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持字符串、浮点数、整数、布尔值、时间戳等基础标量数值类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"scalar_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"str_field\", DataType.VARCHAR, max_length=200),\n",
    "            FieldSchema(\"float_field\", DataType.FLOAT),\n",
    "            FieldSchema(\"int_field\", DataType.INT32),\n",
    "            FieldSchema(\"bool_field\", DataType.BOOL),\n",
    "            FieldSchema(\"ts_field\", DataType.INT64)  # Timestamp stored as INT64\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    data = [\n",
    "        list(range(100)),  # IDs\n",
    "        vectors,  # Vectors\n",
    "        [f\"str_{i}\" + \"a\"*(i%10) for i in range(100)],  # VARCHAR\n",
    "        np.random.rand(100).tolist(),  # FLOAT values\n",
    "        list(np.random.randint(0, 10000, 100)),  # INT32\n",
    "        [bool(i%2) for i in range(100)],  # BOOL\n",
    "        [int(1672531200 + i*3600) for i in range(100)]  # INT64 timestamps (hourly)\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 5. Verify Query\n",
    "    logging.info(\"Querying with scalar conditions...\")\n",
    "    res = collection.query(\n",
    "        # expr=\"id == 0\",\n",
    "        # expr=\"bool_field == True\",\n",
    "        expr=\"float_field > 0.5 and bool_field == True\",\n",
    "        # expr=\"str_field like 'str_1%' and float_field > 0.5 and bool_field == True\",\n",
    "        output_fields=[\"*\"]\n",
    "    )\n",
    "    logging.info(\"Scalar query results:\")\n",
    "    for r in res[:3]:  # logging.info first 3 results\n",
    "        logging.info(f\"ID:{r['id']} | str:{r['str_field']} | float:{r['float_field']:.2f} | \"\n",
    "              f\"int:{r['int_field']} | bool:{r['bool_field']} | ts:{r['ts_field']}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"scalar_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持浮点数向量数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"float_vector_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    collection.insert([list(range(100)), vectors.tolist()])\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 5. Verify Query\n",
    "    logging.info(\"Running query...\")\n",
    "    res = collection.query(expr=\"id == 0\", output_fields=[\"vector\"])\n",
    "    logging.info(\"Query results:\")\n",
    "    logging.info(f\"Query returned {len(res)} results:\")\n",
    "    for idx, result in enumerate(res):\n",
    "        logging.info(f\"Result {idx}: ID={result['id']}, Vector={result['vector'][:5]}...\")  # Show first 5 elements\n",
    "\n",
    "    # 6. Verify Search\n",
    "    logging.info(\"Running search...\")\n",
    "    search_result = collection.search(\n",
    "        vectors[:1].tolist(),\n",
    "        \"vector\",\n",
    "        {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n",
    "        limit=3,\n",
    "        output_fields=[\"vector\"]\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Search results:\")\n",
    "    logging.info(f\"Search returned {len(search_result[0])} results:\")\n",
    "    for idx, hit in enumerate(search_result[0]):\n",
    "        logging.info(f\"Rank {idx+1}: ID={hit.id}, Distance={hit.distance:.4f}\")\n",
    "        logging.info(f\"Vector: {hit.entity.fields['vector'][:5]}...\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"float_vector_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持JSON等至少一种半结构化数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    utility.drop_collection(\"json_test\")\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"json_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"metadata\", DataType.JSON)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    data = [\n",
    "        list(range(100)),\n",
    "        vectors,\n",
    "        [{\n",
    "            \"title\": f\"doc_{i}\",\n",
    "            \"tags\": [\"tag1\", \"tag2\"] if i%2 else [\"tag3\"],\n",
    "            \"stats\": {\"views\": i*10, \"rating\": round(np.random.uniform(1, 5), 1)}\n",
    "        } for i in range(100)]\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 5. Verify JSON query\n",
    "    logging.info(\"Querying JSON data...\")\n",
    "    res = collection.query(\n",
    "        expr=\"metadata['title'] == 'doc_0' and metadata['stats']['rating'] > 0\",\n",
    "        output_fields=[\"metadata\", \"id\"]\n",
    "    )\n",
    "    logging.info(\"JSON query results:\")\n",
    "    for r in res:\n",
    "        logging.info(f\"ID:{r['id']} | Metadata:{r['metadata']}\")\n",
    "\n",
    "    # 6. Verify JSON in search results\n",
    "    search_result = collection.search(\n",
    "        data[1][:1],  # Use first vector\n",
    "        \"vector\",\n",
    "        param={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n",
    "        limit=3,\n",
    "        output_fields=[\"metadata\"]\n",
    "    )\n",
    "    logging.info(\"JSON in search results:\")\n",
    "    for hit in search_result[0]:\n",
    "        logging.info(f\"ID:{hit.id} | Metadata:{hit.entity.fields['metadata']}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"json_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否能在单行数据中支持多个向量字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"multi_vector_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector1\", DataType.FLOAT_VECTOR, dim=256),\n",
    "            FieldSchema(\"vector2\", DataType.FLOAT_VECTOR, dim=128),\n",
    "            FieldSchema(\"metadata\", DataType.JSON)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    index_params = [\n",
    "        (\"vector1\", {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}}),\n",
    "        (\"vector2\", {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"IP\", \"params\": {\"nlist\": 64}})\n",
    "    ]\n",
    "    \n",
    "    for field, params in index_params:\n",
    "        collection.create_index(field, params)\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors1 = np.random.randn(100, 256).astype(np.float32)\n",
    "    vectors2 = np.random.randn(100, 128).astype(np.float32)\n",
    "    data = [\n",
    "        list(range(100)),\n",
    "        np.random.randn(100, 256).tolist(),  # Vector1\n",
    "        np.random.randn(100, 128).tolist(),   # Vector2\n",
    "        [{\"description\": f\"item_{i}\"} for i in range(100)]\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "\n",
    "    logging.info(f\"Inserted {len(vectors1)} vectors\")\n",
    "\n",
    "    # 5. Verify query returns both vectors\n",
    "    res = collection.query(\n",
    "        expr=\"id == 0\",\n",
    "        output_fields=[\"vector1\", \"vector2\"]\n",
    "    )\n",
    "    logging.info(\"Multi-vector query results:\")\n",
    "    for r in res:\n",
    "        logging.info(f\"ID:{r['id']} | Vector1 Head:{r['vector1'][:5]} | Vector2 Head:{r['vector2'][:5]}\")\n",
    "\n",
    "    # 6. Verify search on both vector fields\n",
    "    for vec_field in [\"vector1\", \"vector2\"]:\n",
    "        logging.info(f\"Searching with {vec_field}...\")\n",
    "        search_result = collection.search(\n",
    "            data[1 if vec_field == \"vector1\" else 2][:1],  # Get corresponding vector\n",
    "            vec_field,\n",
    "            param={\"metric_type\": \"L2\" if vec_field == \"vector1\" else \"IP\", \"params\": {\"nprobe\": 10}},\n",
    "            limit=3,\n",
    "            output_fields=[\"metadata\"]\n",
    "        )\n",
    "        logging.info(f\"{vec_field} search results:\")\n",
    "        for hit in search_result[0]:\n",
    "            logging.info(f\"ID:{hit.id} | Distance:{hit.distance:.4f} | Metadata:{hit.entity.fields['metadata']}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"multi_vector_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持向量数据的压缩能力，减少存储空间开销，压缩算法如PQ、float2、zstd等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "    \n",
    "    if utility.has_collection(\"multi_vector_test\"):\n",
    "        utility.drop_collection(\"multi_vector_test\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"multi_vector_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector1\", DataType.FLOAT16_VECTOR, dim=256), #float16\n",
    "            FieldSchema(\"vector2\", DataType.FLOAT_VECTOR, dim=128),\n",
    "            FieldSchema(\"metadata\", DataType.JSON)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    index_params = [\n",
    "        (\"vector1\", {\"index_type\": \"IVF_PQ\", \"metric_type\": \"L2\", \"params\": {\"m\": 4}}),\n",
    "        (\"vector2\", {\"index_type\": \"HNSW_PQ\", \"metric_type\": \"L2\", \"params\": {\"M\": 64, \"efConstruction\": 128}})\n",
    "    ]\n",
    "    \n",
    "    for field, params in index_params:\n",
    "        collection.create_index(field, params)\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors (modified for float16)\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    raw_vectors = []\n",
    "    fp16_vectors = []\n",
    "    # float16, little endian\n",
    "    fp16_little = np.dtype('e').newbyteorder('<')\n",
    "    for _ in range(100):\n",
    "        raw_vector = [np.random.random() for _ in range(256)]\n",
    "        raw_vectors.append(raw_vector)\n",
    "        fp16_vector = np.array(raw_vector, dtype=fp16_little)\n",
    "        fp16_vectors.append(fp16_vector)\n",
    "    vectors2 = np.random.randn(100, 128).astype(np.float32)\n",
    "    \n",
    "    data = [\n",
    "        list(range(100)),\n",
    "        fp16_vectors,  # Use converted bytes for float16\n",
    "        vectors2.tolist(),\n",
    "        [{\"description\": f\"item_{i}\"} for i in range(100)]\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "\n",
    "    # 6. Verify search on both vector fields (modified for float16)\n",
    "    for vec_field in [\"vector1\", \"vector2\"]:\n",
    "        logging.info(f\"Searching with {vec_field}...\")\n",
    "        search_vector = fp16_vectors[0] if vec_field == \"vector1\" else vectors2[0].tolist()\n",
    "        \n",
    "        search_result = collection.search(\n",
    "            [search_vector],\n",
    "            vec_field,\n",
    "            param={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n",
    "            limit=3,\n",
    "            output_fields=[\"metadata\", \"vector1\", \"vector2\"]\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"{vec_field} search results:\")\n",
    "        for r in search_result[0]:\n",
    "            logging.info(f\"ID:{r.id} | Vector1 Head:{np.frombuffer(r.entity.fields['vector1'], dtype=fp16_little)[:5]} | Vector2 Head:{r.entity.fields['vector2'][:5]}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"multi_vector_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持至少一种近似最近邻检索算法（ANNS），如HNSW、IVF等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    utility.drop_collection(\"multi_index_test\")\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"multi_index_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"ivf_flat\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"ivf_pq\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"hnsw\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"diskann\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    index_configs = [\n",
    "        (\"ivf_flat\", {\n",
    "            \"index_type\": \"IVF_FLAT\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"nlist\": 1024}\n",
    "        }),\n",
    "        (\"ivf_pq\", {\n",
    "            \"index_type\": \"IVF_PQ\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"nlist\": 512, \"m\": 16, \"nbits\": 8}\n",
    "        }),\n",
    "        (\"hnsw\", {\n",
    "            \"index_type\": \"HNSW\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"M\": 24, \"efConstruction\": 200}\n",
    "        }),\n",
    "        (\"diskann\", {\n",
    "            \"index_type\": \"DISKANN\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"search_cache_size\": 2, \"build_cache_size\": 4}\n",
    "        })\n",
    "    ]\n",
    "    for field, config in index_configs:\n",
    "        collection.create_index(field, config)\n",
    "    collection.load()\n",
    "\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors1 = np.random.randn(100, dim).astype(np.float32)\n",
    "    vectors2 = np.random.randn(100, dim).astype(np.float32)\n",
    "    vectors3 = np.random.randn(100, dim).astype(np.float32)\n",
    "    vectors4 = np.random.randn(100, dim).astype(np.float32)\n",
    "    data = [\n",
    "        list(range(100)),\n",
    "        vectors1,  # ivf_flat\n",
    "        vectors2,   # ivf_pq\n",
    "        vectors3,   # hnsw\n",
    "        vectors4    # diskann\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "\n",
    "    logging.info(f\"Inserted {len(vectors1)} vectors\")\n",
    "\n",
    "    # 5. Verify search for each index type\n",
    "    search_params = {\n",
    "        \"ivf_flat\": {\"nprobe\": 32},\n",
    "        \"ivf_pq\": {\"nprobe\": 64},\n",
    "        \"hnsw\": {\"ef\": 128},\n",
    "        \"diskann\": {\"search_list\": 100}\n",
    "    }\n",
    "\n",
    "    for field in [\"ivf_flat\", \"ivf_pq\", \"hnsw\", \"diskann\"]:\n",
    "        logging.info(f\"Testing {field.upper()} search...\")\n",
    "        results = collection.search(\n",
    "            data[1][:1] if field == \"ivf_flat\" else \n",
    "            data[2][:1] if field == \"ivf_pq\" else\n",
    "            data[3][:1] if field == \"hnsw\" else\n",
    "            data[4][:1],\n",
    "            field,\n",
    "            param={\"metric_type\": \"L2\", \"params\": search_params[field]},\n",
    "            limit=5,\n",
    "            output_fields=[\"id\"]\n",
    "        )\n",
    "        logging.info(f\"{field} Top 3 results:\")\n",
    "        for hit in results[0][:3]:\n",
    "            logging.info(f\"ID: {hit.id} | Distance: {hit.distance:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"multi_index_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持向量的精确检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"exact_search_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"FLAT\", \"metric_type\": metric_type} \n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    collection.insert([list(range(100)), vectors.tolist()])\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 6. Verify exact search\n",
    "    logging.info(\"Running exact search...\")\n",
    "    search_result = collection.search(\n",
    "        vectors[:1].tolist(),\n",
    "        \"vector\",\n",
    "        {\"metric_type\": metric_type, \"params\": {}},  # Empty params for exact search\n",
    "        limit=100,  # Return all for verification\n",
    "        output_fields=[\"vector\"]\n",
    "    )\n",
    "    \n",
    "    # Verify all results are returned\n",
    "    logging.info(f\"Exact search returned {len(search_result[0])} results\")\n",
    "    logging.info(\"Top 3 matches:\")\n",
    "    for hit in search_result[0][:3]:\n",
    "        logging.info(f\"ID: {hit.id} | Distance: {hit.distance:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"exact_search_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持向量索引（包括近似索引和精确索引）和标量过滤同时检索，标量过滤应支持常用的运算符，包含比较运算符（=,<>,>,>=,<,<=）和逻辑运算符（与、或、非）等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"hybrid_search_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"str_field\", DataType.VARCHAR, max_length=200),\n",
    "            FieldSchema(\"float_field\", DataType.FLOAT),\n",
    "            FieldSchema(\"int_field\", DataType.INT32),\n",
    "            FieldSchema(\"bool_field\", DataType.BOOL),\n",
    "            FieldSchema(\"ts_field\", DataType.INT64)  # Timestamp stored as INT64\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    data = [\n",
    "        list(range(100)),  # IDs\n",
    "        vectors,  # Vectors\n",
    "        [f\"str_{i}\" + \"a\"*(i%10) for i in range(100)],  # VARCHAR\n",
    "        np.random.rand(100).tolist(),  # FLOAT values\n",
    "        list(np.random.randint(0, 10000, 100)),  # INT32\n",
    "        [bool(i%2) for i in range(100)],  # BOOL\n",
    "        [int(1672531200 + i*3600) for i in range(100)]  # INT64 timestamps (hourly)\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 5. Enhanced query validation with various operators\n",
    "    logging.info(\"Testing scalar filtering operators...\")\n",
    "    \n",
    "    # Test cases with different operators\n",
    "    test_cases = [\n",
    "        (\"Basic comparison\", \"int_field > 5000 and float_field <= 0.8\"),\n",
    "        (\"Equality check\", \"str_field == 'str_5aaaaa' or bool_field == false\"),\n",
    "        (\"Range filter\", \"ts_field >= 1672531200 and ts_field < 1672617600\"),\n",
    "        (\"Negation\", \"not (int_field in [100, 200, 300])\"),\n",
    "        (\"Complex logic\", \"(0.3 <= float_field <= 0.7) and (str_field like 'str_2%')\")\n",
    "    ]\n",
    "    \n",
    "    for desc, expr in test_cases:\n",
    "        logging.info(f\"Testing: {desc}\")\n",
    "        res = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=[\"*\"]\n",
    "        )\n",
    "        logging.info(f\"Expression: {expr}\")\n",
    "        logging.info(f\"Matched {len(res)} records\")\n",
    "        if res:\n",
    "            logging.info(f\"First result - ID:{res[0]['id']} int:{res[0]['int_field']} float:{res[0]['float_field']:.2f}\")\n",
    "\n",
    "    # 6. Validate vector search with scalar filtering\n",
    "    logging.info(\"Testing combined vector search and scalar filtering...\")\n",
    "    search_result = collection.search(\n",
    "        vectors[:1],  # Use first vector\n",
    "        \"vector\",\n",
    "        param={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n",
    "        expr=\"int_field < 5000 and bool_field == true\",\n",
    "        limit=5,\n",
    "        output_fields=[\"*\"]\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Vector search with scalar filter results:\")\n",
    "    for hit in search_result[0]:\n",
    "        logging.info(f\"ID:{hit.id} | Distance:{hit.distance:.4f} | int:{hit.entity.fields['int_field']} | bool:{hit.entity.fields['bool_field']}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"hybrid_search_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持对标量数据增删改查操作的原子性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 64\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"insert_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"price\", DataType.INT64)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "\n",
    "    # 1. Test atomic insert\n",
    "    try:\n",
    "        insert_ids = [10001]\n",
    "        insert_vectors = [np.random.randn(dim).tolist()]\n",
    "        insert_prices = [999]\n",
    "        collection.insert([insert_ids, insert_vectors, insert_prices])\n",
    "        res = collection.query(expr=\"id == 10001\", output_fields=[\"price\"])\n",
    "        assert res[0][\"price\"] == 999, \"Insert atomicity failed\"\n",
    "        logging.info(\"Atomic insert validated\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Insert atomicity test failed: {str(e)}\")\n",
    "\n",
    "    # 2. Test atomic update\n",
    "    try:\n",
    "        original_price = collection.query(expr=\"id == 10001\", output_fields=[\"price\"])[0][\"price\"]\n",
    "        \n",
    "        # Update price and vector simultaneously\n",
    "        collection.upsert(\n",
    "            data=[{\n",
    "                \"id\": 10001,\n",
    "                \"vector\": np.random.randn(dim).tolist(),\n",
    "                \"price\": original_price + 100\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        updated = collection.query(expr=\"id == 10001\", output_fields=[\"price\", \"vector\"])\n",
    "        assert updated[0][\"price\"] == original_price + 100, \"Update atomicity failed\"\n",
    "        logging.info(\"Atomic update validated\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Update atomicity test failed: {str(e)}\")\n",
    "\n",
    "    # 3. Test atomic delete\n",
    "    try:\n",
    "        # Delete record and check both scalar/vector removal\n",
    "        collection.delete(expr=\"id == 10001\")\n",
    "        res = collection.query(expr=\"id == 10001\")\n",
    "        assert len(res) == 0, \"Delete atomicity failed\"\n",
    "        logging.info(\"Atomic delete validated\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Delete atomicity test failed: {str(e)}\")\n",
    "\n",
    "    # 4. Test failed operation rollback\n",
    "    try:\n",
    "        # Attempt invalid upsert\n",
    "        collection.upsert(data=[{\"id\": 10001, \"price\": 100}])\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Expected error occurred: {str(e)}\")\n",
    "        res = collection.query(expr=\"id == 10001\")\n",
    "        assert len(res) == 0, \"Failed operation should rollback\"\n",
    "        logging.info(\"Operation rollback validated\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"insert_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持对向量数据增删改查操作的原子性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 64\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"insert_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "\n",
    "    # Insert atomicity test data\n",
    "    test_id = 9999\n",
    "    original_vector = [0.1] * dim\n",
    "    updated_vector = [0.9] * dim\n",
    "\n",
    "    # 1. Test atomic insert\n",
    "    try:\n",
    "        # Insert vector with metadata\n",
    "        collection.insert([[test_id], [original_vector]])\n",
    "        \n",
    "        # Immediate query verification\n",
    "        res = collection.query(expr=f\"id == {test_id}\", output_fields=[\"vector\"])\n",
    "        assert res[0][\"vector\"] == original_vector, \"Inserted vector mismatch\"\n",
    "        logging.info(\"Atomic insert validated\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Insert atomicity failed: {str(e)}\")\n",
    "\n",
    "    # 2. Test atomic update \n",
    "    try:\n",
    "        # Update vector content\n",
    "        collection.upsert([[test_id], [updated_vector]])\n",
    "        \n",
    "        # Verify complete replacement\n",
    "        res = collection.query(expr=f\"id == {test_id}\", output_fields=[\"vector\"])\n",
    "        assert res[0][\"vector\"] == updated_vector, \"Vector update incomplete\"\n",
    "        logging.info(\"Atomic update validated\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Update atomicity failed: {str(e)}\")\n",
    "\n",
    "    # 3. Test atomic delete\n",
    "    try:\n",
    "        # Delete and verify removal\n",
    "        collection.delete(expr=f\"id == {test_id}\")\n",
    "        res = collection.query(expr=f\"id == {test_id}\")\n",
    "        assert len(res) == 0, \"Vector still exists after deletion\"\n",
    "        logging.info(\"Atomic delete validated\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Delete atomicity failed: {str(e)}\")\n",
    "\n",
    "    # 4. Test bulk operation atomicity\n",
    "    try:\n",
    "        # Batch insert with invalid vector (should rollback)\n",
    "        invalid_data = [\n",
    "            [10001, 10002],\n",
    "            [np.random.rand(128).tolist()],  # 错误的维度\n",
    "            [{\"status\": \"active\"}, {\"status\": \"pending\"}]\n",
    "        ]\n",
    "        collection.insert(invalid_data)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Expected error occurred: {str(e)}\")\n",
    "        res = collection.query(expr=\"id in [10001, 10002]\")\n",
    "        assert len(res) == 0, \"Partial insert detected\"\n",
    "        logging.info(\"Bulk operation rollback validated\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"insert_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持根据主键查询对应的标量和向量数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"pk_query_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"title\", DataType.VARCHAR, max_length=200),\n",
    "            FieldSchema(\"price\", DataType.DOUBLE)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"FLAT\", \"metric_type\": metric_type} \n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    data = [\n",
    "        list(range(100)),  # IDs\n",
    "        vectors.tolist(),  # Vectors\n",
    "        [f\"Product_{i}\" for i in range(100)],  # Titles\n",
    "        np.random.uniform(1, 1000, 100).tolist()  # Prices\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 5. Verify primary key query\n",
    "    logging.info(\"Testing primary key lookup...\")\n",
    "    test_ids = [0, 50, 99]\n",
    "    \n",
    "    for pk in test_ids:\n",
    "        result = collection.query(\n",
    "            expr=f\"id == {pk}\",\n",
    "            output_fields=[\"*\"]  # Retrieve all fields\n",
    "        )\n",
    "        if result:\n",
    "            item = result[0]\n",
    "            logging.info(f\"ID {pk} details: Title: {item['title']} | Price: ${item['price']:.2f} | Vector: {item['vector'][:5]}...\")\n",
    "        else:\n",
    "            logging.warning(f\"No result found for ID {pk}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"pk_query_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持按单条与批量导入方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"insert_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    # Batch insert (100 vectors)\n",
    "    batch_ids = list(range(100))\n",
    "    batch_vectors = np.random.randn(100, dim).astype(np.float32).tolist()\n",
    "    collection.insert([batch_ids, batch_vectors])\n",
    "    logging.info(\"Inserted 100 vectors in batch\")\n",
    "\n",
    "    # Single insert\n",
    "    single_id = [100]\n",
    "    single_vector = [np.random.randn(dim).astype(np.float32).tolist()]\n",
    "    collection.insert([single_id, single_vector])\n",
    "    logging.info(\"Inserted 1 vector individually\")\n",
    "    \n",
    "    # Small batch insert\n",
    "    small_batch_ids = [101, 102, 103]\n",
    "    small_batch_vectors = np.random.randn(3, dim).astype(np.float32).tolist()\n",
    "    collection.insert([small_batch_ids, small_batch_vectors])\n",
    "    logging.info(\"Inserted 3 vectors in small batch\")\n",
    "\n",
    "    # Verify counts\n",
    "    res = collection.query(\n",
    "        expr=\"id >= 0\",  # Match all records\n",
    "        output_fields=[\"count(*)\"],\n",
    "        count=True\n",
    "    )\n",
    "    actual_count = res[0][\"count(*)\"]\n",
    "    logging.info(f\"Count(*) result: {actual_count} (Expected: 104)\")\n",
    "\n",
    "    # 5. Verify different insertions\n",
    "    test_ids = [0, 100, 101]\n",
    "    for tid in test_ids:\n",
    "        res = collection.query(\n",
    "            expr=f\"id == {tid}\",\n",
    "            output_fields=[\"vector\"]\n",
    "        )\n",
    "        status = \"Found\" if res else \"Missing\"\n",
    "        logging.info(f\"Verification: ID {tid} - {status}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"insert_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "from pymilvus.bulk_writer import bulk_import, list_import_jobs, RemoteBulkWriter, BulkFileType\n",
    "import json, time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "    \n",
    "    collection_name = \"bulk_import_test\"\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    \n",
    "    \n",
    "    schema = CollectionSchema([\n",
    "        FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "        FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=256),\n",
    "    ])\n",
    "    collection = Collection(\n",
    "        collection_name,\n",
    "        schema,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "    \n",
    "    # Third-party constants\n",
    "    ACCESS_KEY=\"minioadmin\"\n",
    "    SECRET_KEY=\"minioadmin\"\n",
    "    BUCKET_NAME=\"a-bucket\"\n",
    "    \n",
    "    minio_endpoint = \"localhost:9000\"  # the default MinIO service started along with Milvus\n",
    "    remote_path = \"/bulk_data/\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    url = f\"http://127.0.0.1:19530\"\n",
    "\n",
    "    # Connections parameters to access the remote bucket\n",
    "    conn = RemoteBulkWriter.S3ConnectParam(\n",
    "        endpoint=minio_endpoint,\n",
    "        access_key=ACCESS_KEY,\n",
    "        secret_key=SECRET_KEY,\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "    writer = RemoteBulkWriter(\n",
    "        schema=schema,\n",
    "        remote_path=remote_path,\n",
    "        connect_param=conn,\n",
    "        file_type=BulkFileType.PARQUET\n",
    "    )\n",
    "    print('bulk writer created.')\n",
    "    \n",
    "    for i in range(10000):\n",
    "        writer.append_row({\n",
    "            \"id\": i,\n",
    "            \"vector\": np.random.randn(256).astype(np.float32).tolist(),\n",
    "        })\n",
    "        \n",
    "        if i+1 % 1000 == 0:\n",
    "            writer.commit()\n",
    "            print(f'bulk writer flushed {i} rows.')\n",
    "            \n",
    "    writer.commit()\n",
    "    print('bulk writer flushed all rows.')\n",
    "    print(writer.batch_files)\n",
    "    \n",
    "    resp = bulk_import(\n",
    "        url,\n",
    "        collection_name,\n",
    "        files=writer.batch_files,\n",
    "    )\n",
    "\n",
    "    job_id = resp.json()['data']['jobId']\n",
    "    print(f'bulk import job id: {job_id}')\n",
    "    \n",
    "    progress = 0\n",
    "    while True:\n",
    "        resp = list_import_jobs(\n",
    "            url=url,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        new_progress = resp.json()['data']['records'][0]['progress']\n",
    "        if new_progress > progress:\n",
    "            progress = new_progress\n",
    "            print(json.dumps(resp.json(), indent=4))\n",
    "        \n",
    "        if (resp.json()['data']['records'][0]['jobId'] == job_id) and (new_progress== 100):\n",
    "            break\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    index_params = [\n",
    "        (\"vector\", {\"index_type\": \"HNSW\", \"metric_type\": \"L2\", \"params\": {\"M\": 64, \"efConstruction\": 128}})\n",
    "    ]\n",
    "    \n",
    "    for field, params in index_params:\n",
    "        collection.create_index(field, params)\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "    \n",
    "    # verify count(*) result\n",
    "    res = collection.query(\n",
    "        expr=\"id >= 0\",  # Match all records\n",
    "        output_fields=[\"count(*)\"],\n",
    "        count=True\n",
    "    )\n",
    "    actual_count = res[0][\"count(*)\"]\n",
    "    logging.info(f\"Count(*) result: {actual_count} (Expected: 10000)\") \n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持按批量等至少一种导出方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 64\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"insert_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    # Batch insert (100 vectors)\n",
    "    batch_ids = list(range(10000))\n",
    "    batch_vectors = np.random.randn(10000, dim).astype(np.float32).tolist()\n",
    "    collection.insert([batch_ids, batch_vectors])\n",
    "    logging.info(\"Inserted 100 vectors in batch\")\n",
    "\n",
    "    # Add search iterator validation\n",
    "    logging.info(\"\\n=== Testing Search Iterator ===\")\n",
    "    export_results = []\n",
    "    batch_size = 1000\n",
    "    limit = 10000\n",
    "    \n",
    "    \n",
    "    iterator = collection.search_iterator(\n",
    "            data=[np.random.randn(dim).tolist()],  # Random query vector\n",
    "            anns_field=\"vector\",\n",
    "            param={\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n",
    "            batch_size=batch_size,\n",
    "            limit=limit,\n",
    "            output_fields=[\"vector\"],\n",
    "            expr=\"id >= 0\"  # Export all data\n",
    "        )\n",
    "    \n",
    "    while True:\n",
    "        result = iterator.next()\n",
    "        if not result:\n",
    "            iterator.close()\n",
    "            logging.info(\"Search iterator closed\")\n",
    "            break\n",
    "        \n",
    "        for hit in result:\n",
    "            export_results.append(hit.to_dict())\n",
    "        logging.info(f\"Exported {len(result)} records in batch {len(export_results)//batch_size}\")\n",
    "\n",
    "    logging.info(f\"Total exported records: {len(export_results)}\")\n",
    "    logging.info(f\"Sample exported data: {export_results[:1]}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"insert_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持内积、欧氏距离、余弦等至少三种基础相似距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    metrics = [\n",
    "        (\"L2\", \"L2\"),\n",
    "        (\"IP\", \"IP\"),\n",
    "        (\"COSINE\", \"L2\")  # Cosine uses L2 with normalized vectors\n",
    "    ]\n",
    "    dim = 768\n",
    "    for metric_name, metric_type in metrics:\n",
    "        coll_name = f\"metric_test_{metric_name}\"\n",
    "        logging.info(f\"Creating {metric_name} collection...\")\n",
    "        \n",
    "        # Create collection\n",
    "        coll = Collection(\n",
    "            coll_name,\n",
    "            CollectionSchema([\n",
    "                FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "                FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "            ]),\n",
    "            consistency_level=\"Strong\"\n",
    "        )\n",
    "\n",
    "        # Generate vectors (normalize for cosine)\n",
    "        if metric_name == \"COSINE\":\n",
    "            vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "            norms = np.linalg.norm(vectors, axis=1)\n",
    "            vectors = vectors / norms[:, np.newaxis]\n",
    "        else:\n",
    "            vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "        \n",
    "        # Insert data\n",
    "        coll.insert([list(range(100)), vectors.tolist()])\n",
    "        logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "        # Create index\n",
    "        coll.create_index(\n",
    "            \"vector\",\n",
    "            {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "        )\n",
    "        coll.load()\n",
    "\n",
    "        # Search validation\n",
    "        logging.info(f\"Testing {metric_name} similarity...\")\n",
    "        search_vec = [vectors[0].tolist()]  # Query with first vector\n",
    "        results = coll.search(\n",
    "            search_vec,\n",
    "            \"vector\",\n",
    "            {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n",
    "            limit=3,\n",
    "            output_fields=[\"vector\"]\n",
    "        )\n",
    "\n",
    "        # logging.info top result distances\n",
    "        logging.info(f\"{metric_name} results:\")\n",
    "        for hit in results[0]:\n",
    "            logging.info(f\"ID: {hit.id} | Distance: {hit.distance:.4f}\")\n",
    "\n",
    "        # Cleanup\n",
    "        utility.drop_collection(coll_name)\n",
    "\n",
    "    logging.info(\"All metric tests completed\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持汉明距离、Jaccard相似系数、曼哈顿距离等至少一种二进制相似度计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # Add binary vector validation\n",
    "    bin_dim = 1024  # 1024 bits = 128 bytes\n",
    "    binary_metrics = [\"HAMMING\", \"JACCARD\"]\n",
    "    \n",
    "    for b_metric in binary_metrics:\n",
    "        coll_name = f\"binary_test_{b_metric}\"\n",
    "        logging.info(f\"=== Testing {b_metric} ===\")\n",
    "\n",
    "        # Create binary collection\n",
    "        bin_col = Collection(\n",
    "            coll_name,\n",
    "            CollectionSchema([\n",
    "                FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "                FieldSchema(\"vector\", DataType.BINARY_VECTOR, dim=bin_dim)\n",
    "            ]),\n",
    "            consistency_level=\"Strong\"\n",
    "        )\n",
    "        \n",
    "        # Generate binary vectors (128 bytes)\n",
    "        byte_len = bin_dim // 8\n",
    "        vectors = [bytes(np.random.bytes(byte_len)) for _ in range(100)]\n",
    "        \n",
    "        # Insert data\n",
    "        bin_col.insert([list(range(100)), vectors])\n",
    "        \n",
    "        # Create index\n",
    "        bin_col.create_index(\n",
    "            \"vector\",\n",
    "            {\"index_type\": \"BIN_IVF_FLAT\", \"metric_type\": b_metric, \"params\": {\"nlist\": 16}}\n",
    "        )\n",
    "        bin_col.load()\n",
    "        \n",
    "        # Search validation\n",
    "        results = bin_col.search(\n",
    "            [vectors[0]], \n",
    "            \"vector\", \n",
    "            {\"metric_type\": b_metric, \"params\": {\"nprobe\": 10}}, \n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"{b_metric} Results:\")\n",
    "        for hit in results[0]:\n",
    "            logging.info(f\"ID: {hit.id} | Distance: {hit.distance}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        utility.drop_collection(coll_name)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持至少一种索引的构建与删除，标量索引如布尔、字符、数字标量索引，向量索引如HNSW、IVF、LSH等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"index_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=False\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"bool_field\", datatype=DataType.BOOL)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=100)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection with schema\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"IVF_FLAT\", \"L2\", {\"nlist\": 16}),\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    for field_name, idx_name, metric, params in vector_indexes:\n",
    "        logging.info(f\"Testing {idx_name} vector index:\")\n",
    "        \n",
    "        # Prepare index parameters\n",
    "        index_params = client.prepare_index_params()\n",
    "        index_params.add_index(\n",
    "            field_name=field_name,\n",
    "            index_type=idx_name,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_name}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Create index\n",
    "        client.create_index(\n",
    "            collection_name=collection_name,\n",
    "            index_params=index_params,\n",
    "            sync=False\n",
    "        )\n",
    "        logging.info(f\"Created {idx_name} index\")\n",
    "        \n",
    "        # Add index description logging\n",
    "        desc = client.describe_index(\n",
    "            collection_name=collection_name,\n",
    "            index_name=f\"vector_{idx_name}_index\"\n",
    "        )\n",
    "        logging.info(f\"Vector index description: {desc}\")\n",
    "        \n",
    "        client.drop_index(\n",
    "            collection_name=collection_name,\n",
    "            index_name=f\"vector_{idx_name}_index\"\n",
    "        )\n",
    "        logging.info(f\"Dropped {idx_name} index\")\n",
    "\n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"bool_field\", \"INVERTED\"),\n",
    "        (\"string_field\", \"TRIE\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        logging.info(f\"Testing {idx_type} index for {field}:\")\n",
    "        \n",
    "        # Prepare scalar index parameters\n",
    "        index_params = client.prepare_index_params()\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\"\n",
    "        )\n",
    "        \n",
    "        client.create_index(\n",
    "            collection_name=collection_name,\n",
    "            index_params=index_params\n",
    "        )\n",
    "        logging.info(f\"Created {idx_type} index\")\n",
    "        \n",
    "        # Add scalar index description logging\n",
    "        desc = client.describe_index(\n",
    "            collection_name=collection_name,\n",
    "            index_name=f\"{field}_index\"\n",
    "        )\n",
    "        logging.info(f\"Scalar index description: {desc}\")\n",
    "        \n",
    "        client.drop_index(\n",
    "            collection_name=collection_name,\n",
    "            index_name=f\"{field}_index\"\n",
    "        )\n",
    "        logging.info(f\"Dropped {idx_type} index\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持增量索引功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"dense_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(\"payload\", DataType.INT64)\n",
    "        ]),\n",
    "        consistency_level=\"Strong\" \n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    collection.create_index(\n",
    "        \"vector\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": metric_type, \"params\": {\"nlist\": 16}}\n",
    "    )\n",
    "    collection.load()\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors = np.random.randn(100, dim).astype(np.float32)\n",
    "    collection.insert([list(range(100)), vectors.tolist(), list(range(100))])\n",
    "    logging.info(f\"Inserted {len(vectors)} vectors\")\n",
    "\n",
    "    # 5. Verify Query\n",
    "    logging.info(\"Running query...\")\n",
    "    res = collection.query(expr=\"id == 0\", output_fields=[\"vector\"])\n",
    "    logging.info(\"Query results:\")\n",
    "    logging.info(f\"Query returned {len(res)} results:\")\n",
    "    for idx, result in enumerate(res):\n",
    "        logging.info(f\"Result {idx}: ID={result['id']}, Vector={result['vector'][:5]}...\")  # Show first 5 elements\n",
    "\n",
    "    # 6. Verify Search\n",
    "    logging.info(\"Running search...\")\n",
    "    search_result = collection.search(\n",
    "        vectors[:1].tolist(),\n",
    "        \"vector\",\n",
    "        {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n",
    "        limit=3,\n",
    "        output_fields=[\"vector\", \"payload\"]\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Search results:\")\n",
    "    logging.info(f\"Search returned {len(search_result[0])} results:\")\n",
    "    for idx, hit in enumerate(search_result[0]):\n",
    "        logging.info(f\"Rank {idx+1}: ID={hit.id}, Distance={hit.distance:.4f}, Payload={hit.entity.fields['payload']}, Vector: {hit.entity.fields['vector'][:5]}...\")\n",
    "    \n",
    "    # 7. Insert additional vectors for incremental index test\n",
    "    logging.info(\"Inserting additional vectors for incremental index test...\")\n",
    "    new_vectors = np.random.randn(10, dim).astype(np.float32).tolist()\n",
    "    collection.insert([list(range(100, 110)), new_vectors, list(range(100, 110))])\n",
    "    logging.info(f\"Inserted {len(new_vectors)} additional vectors\")\n",
    "\n",
    "    # 8. Verify incremental index functionality\n",
    "    logging.info(\"Running search after incremental insertion...\")\n",
    "    incremental_search_result = collection.search(\n",
    "        new_vectors[:1],\n",
    "        \"vector\",\n",
    "        {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n",
    "        limit=3,\n",
    "        expr=\"payload > 100\",\n",
    "        output_fields=[\"vector\", \"payload\"]\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Incremental search results:\")\n",
    "    for idx, hit in enumerate(incremental_search_result[0]):\n",
    "        logging.info(f\"Rank {idx+1}: ID={hit.id}, Distance={hit.distance:.4f}, Payload={hit.entity.fields['payload']},  Vector: {hit.entity.fields['vector'][:5]}...\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"dense_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持如强一致性、最终一致性、会话一致性、有界一致性等至少一种数据一致性；应提供数据一致性的明确说明，最终一致性需提供数据延迟的明确说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=False\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"bool_field\", datatype=DataType.BOOL)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=100)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection with schema\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=\"vector\",\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"bool_field\", \"INVERTED\"),\n",
    "        (\"string_field\", \"TRIE\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\"\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params\n",
    "    )\n",
    "    client.load_collection(collection_name)\n",
    "    \n",
    "     # Insert test data for consistency validation\n",
    "    test_vector = np.random.rand(dim).tolist()\n",
    "    test_data = [{\n",
    "        \"id\": 1001,\n",
    "        \"vector\": test_vector,\n",
    "        \"bool_field\": True,\n",
    "        \"string_field\": \"consistency_test\",\n",
    "        \"int_field\": 2024\n",
    "    }]\n",
    "    insert_res = client.insert(\n",
    "        collection_name=collection_name,\n",
    "        data=test_data\n",
    "    )\n",
    "    logging.info(f\"Inserted test data with ID: {insert_res['ids'][0]}\")\n",
    "\n",
    "    # Validate data consistency\n",
    "    logging.info(\"\\n=== Data Consistency Validation ===\")\n",
    "    # consistency_level=\"Eventually\"\n",
    "    # consistency_level=\"Strong\"\n",
    "    # consistency_level=\"Session\"\n",
    "    consistency_level=\"Bounded\"\n",
    "    for i in range(1, 10):\n",
    "        result = client.search(\n",
    "            collection_name=collection_name,\n",
    "            data=[test_vector],\n",
    "            limit=1,\n",
    "            consistency_level=consistency_level,\n",
    "            output_fields=[\"id\"]\n",
    "        )\n",
    "        if result and result[0]:\n",
    "            logging.info(f\"consistency achieved after {i} seconds: {result}\")\n",
    "            break\n",
    "        else:\n",
    "            logging.info(f\"consistency not achieved after {i} seconds\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持利用标量和枚举字段等方式对检索过程进行数据筛选过滤和重排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=False\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"bool_field\", datatype=DataType.BOOL)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=100)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection with schema\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=\"vector\",\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"bool_field\", \"INVERTED\"),\n",
    "        (\"string_field\", \"TRIE\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\"\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params\n",
    "    )\n",
    "    client.load_collection(collection_name)\n",
    "\n",
    "    # Insert test data for filtering/reranking\n",
    "    vector = np.random.rand(dim).tolist()\n",
    "    test_data = [\n",
    "        {\"id\": 1001, \"vector\":  vector, \"bool_field\": True, \"string_field\": \"apple\", \"int_field\": 100},\n",
    "        {\"id\": 1002, \"vector\":  vector, \"bool_field\": False, \"string_field\": \"banana\", \"int_field\": 200},\n",
    "        {\"id\": 1003, \"vector\":  vector, \"bool_field\": True, \"string_field\": \"cherry\", \"int_field\": 300}\n",
    "    ]\n",
    "    client.insert(collection_name, test_data)\n",
    "    logging.info(\"Inserted 3 test documents with different scalar values\")\n",
    "\n",
    "    # Filtering and reranking validation\n",
    "    logging.info(\"=== Scalar Filtering & Sort Validation ===\")\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[vector],\n",
    "        filter=\"string_field in ['apple', 'cherry']\",\n",
    "        output_fields=[\"id\", \"string_field\", \"int_field\"],\n",
    "        # order_by=\"int_field desc\", // not support for now\n",
    "    )\n",
    "    \n",
    "    for result in results[0]:\n",
    "        logging.info(f\"Result: {result}\")\n",
    "    \n",
    "     # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持Schemaless或强Schema。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"bool_field\", datatype=DataType.BOOL)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=100)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection with schema\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=\"vector\",\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"bool_field\", \"INVERTED\"),\n",
    "        (\"string_field\", \"TRIE\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\"\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params\n",
    "    )\n",
    "    client.load_collection(collection_name)\n",
    "\n",
    "    # Insert test data for filtering/reranking\n",
    "    vector = np.random.rand(dim).tolist()\n",
    "    test_data = [\n",
    "        {\"id\": 1001, \"vector\":  vector, \"bool_field\": True, \"string_field\": \"apple\", \"int_field\": 100},\n",
    "        {\"id\": 1002, \"vector\":  vector, \"bool_field\": False, \"string_field\": \"banana\", \"int_field\": 200},\n",
    "        {\"id\": 1003, \"vector\":  vector, \"bool_field\": True, \"string_field\": \"cherry\", \"int_field\": 300}\n",
    "    ]\n",
    "    client.insert(collection_name, test_data)\n",
    "    logging.info(\"Inserted 3 test documents with different scalar values\")\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Testing STRONG SCHEMA...\")\n",
    "        # Attempt to insert data with undefined field\n",
    "        client.insert(\n",
    "            collection_name=collection_name,\n",
    "            data=[{\"id\": 9999, \"vector\": vector, \"undefined_field\": \"invalid\"}]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Strong schema validation working: {str(e)}\")\n",
    "\n",
    "    # Insert document with dynamic field\n",
    "    client.insert(\n",
    "        collection_name,\n",
    "        data=[{\"id\": 10000, \"vector\": vector, \"bool_field\": True, \"string_field\": \"apple\", \"int_field\": 100, \"test_field\": \"test_value\", \"test_field2\": \"test_value2\"}]\n",
    "    )\n",
    "    # Query dynamic field\n",
    "    res = client.search(\n",
    "        collection_name,\n",
    "        data=[vector],\n",
    "        filter=\"id == 10000\",\n",
    "        output_fields=[\"test_field\", \"test_field2\"],\n",
    "        consistency_level=\"Strong\",\n",
    "    )\n",
    "    logging.info(f\"Dynamic field value: {res[0][0]}\")\n",
    "    \n",
    "     # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持如按行、按表、按分区等至少一种粒度的数据TTL（Time to live）管理能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"bool_field\", datatype=DataType.BOOL)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=100)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection with TTL\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        properties={\n",
    "            \"collection.ttl.seconds\": 5,  # 5-second TTL\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=\"vector\",\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"bool_field\", \"INVERTED\"),\n",
    "        (\"string_field\", \"TRIE\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\"\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params,\n",
    "    )\n",
    "    client.load_collection(collection_name)\n",
    "\n",
    "    # Insert test data for filtering/reranking\n",
    "    vector = np.random.rand(dim).tolist()\n",
    "    test_data = [\n",
    "        {\"id\": 1001, \"vector\":  vector, \"bool_field\": True, \"string_field\": \"apple\", \"int_field\": 100},\n",
    "        {\"id\": 1002, \"vector\":  vector, \"bool_field\": False, \"string_field\": \"banana\", \"int_field\": 200},\n",
    "        {\"id\": 1003, \"vector\":  vector, \"bool_field\": True, \"string_field\": \"cherry\", \"int_field\": 300}\n",
    "    ]\n",
    "    client.insert(collection_name, test_data)\n",
    "    client.flush(collection_name)\n",
    "    logging.info(\"Inserted 3 test documents with different scalar values\")\n",
    "\n",
    "\n",
    "    # Immediate query should return both\n",
    "    res = client.query(collection_name, filter=\"id in [1001,1002, 1003]\")\n",
    "    logging.info(f\"Initial records: {len(res)} (should be 3)\")\n",
    "    \n",
    "    # Wait for TTL cleanup\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Wait for compact to clean expired data\n",
    "    client.compact(collection_name)\n",
    "    time.sleep(30)\n",
    "    \n",
    "    # Post-TTL query should only keep valid record\n",
    "    res = client.query(collection_name, filter=\"id in [1001,1002, 1003]\")\n",
    "    logging.info(f\"Post-TTL records: {len(res)} (should be 0)\")\n",
    "    \n",
    "     # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持建立全文索引以及支持全文检索的能力，并支持全文、向量融合检索的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(\n",
    "        field_name=\"string_field\",\n",
    "        datatype=DataType.VARCHAR,\n",
    "        max_length=1000,\n",
    "        enable_analyzer=True,\n",
    "        enable_match=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=\"vector\",\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"string_field\", \"INVERTED\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\",\n",
    "            params=params\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    client.load_collection(collection_name)\n",
    "    \n",
    "\n",
    "    # Text match validation\n",
    "    logging.info(\"\\n=== Text Match Validation ===\")\n",
    "    vector = np.random.rand(dim).tolist()\n",
    "    test_text_data = [\n",
    "        {\"id\": 2001, \"vector\": vector, \"string_field\": \"machine learning system\", \"int_field\": 100},\n",
    "        {\"id\": 2002, \"vector\": vector, \"string_field\": \"deep neural network\", \"int_field\": 200},\n",
    "        {\"id\": 2003, \"vector\": vector, \"string_field\": \"computer vision model\", \"int_field\": 300}\n",
    "    ]\n",
    "    client.insert(collection_name, test_text_data)\n",
    "    client.flush(collection_name)\n",
    "\n",
    "    # Text match search\n",
    "    text_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[vector],\n",
    "        filter=\"TEXT_MATCH(string_field, 'machine deep')\",\n",
    "        output_fields=[\"string_field\"],\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Text match results: {text_results[0]}\")\n",
    "        \n",
    "     # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持多元化存储方式，既可以存储在磁盘或对象存储，也可以加载到内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "try:\n",
    "    # 1. Connect to Milvus\n",
    "    logging.info(\"Connecting to Milvus server...\")\n",
    "    connections.connect(host='127.0.0.1', port='19530')\n",
    "    logging.info(\"Successfully connected to Milvus\")\n",
    "\n",
    "    utility.drop_collection(\"multi_index_test\")\n",
    "    # 2. Create dense vector collection\n",
    "    logging.info(\"Creating collection...\")\n",
    "    dim = 768\n",
    "    metric_type = \"L2\"  # or \"IP\"\n",
    "    collection = Collection(\n",
    "        \"multi_index_test\",\n",
    "        CollectionSchema([\n",
    "            FieldSchema(\"id\", DataType.INT64, is_primary=True),\n",
    "            FieldSchema(\"ivf_flat\", DataType.FLOAT_VECTOR, dim=dim), # in memory index\n",
    "            FieldSchema(\"ivf_pq\", DataType.FLOAT_VECTOR, dim=dim),   # in memory index\n",
    "            FieldSchema(\"hnsw\", DataType.FLOAT_VECTOR, dim=dim),     # in memory index\n",
    "            FieldSchema(\"diskann\", DataType.FLOAT_VECTOR, dim=dim)   # on disk index\n",
    "        ]),\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    logging.info(f\"Collection created: {collection.name}\")\n",
    "\n",
    "    # 3. Create index and load\n",
    "    logging.info(\"Creating index...\")\n",
    "    index_configs = [\n",
    "        (\"ivf_flat\", {\n",
    "            \"index_type\": \"IVF_FLAT\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"nlist\": 1024}\n",
    "        }),\n",
    "        (\"ivf_pq\", {\n",
    "            \"index_type\": \"IVF_PQ\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"nlist\": 512, \"m\": 16, \"nbits\": 8}\n",
    "        }),\n",
    "        (\"hnsw\", {\n",
    "            \"index_type\": \"HNSW\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"M\": 24, \"efConstruction\": 200}\n",
    "        }),\n",
    "        (\"diskann\", {\n",
    "            \"index_type\": \"DISKANN\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"search_cache_size\": 2, \"build_cache_size\": 4}\n",
    "        })\n",
    "    ]\n",
    "    for field, config in index_configs:\n",
    "        collection.create_index(field, config)\n",
    "    collection.load()  # data is loaded into memory before accessing\n",
    "\n",
    "    logging.info(\"Index created and collection loaded\")\n",
    "\n",
    "    # 4. Insert dense vectors\n",
    "    logging.info(\"Generating and inserting vectors...\")\n",
    "    vectors1 = np.random.randn(100, dim).astype(np.float32)\n",
    "    vectors2 = np.random.randn(100, dim).astype(np.float32)\n",
    "    vectors3 = np.random.randn(100, dim).astype(np.float32)\n",
    "    vectors4 = np.random.randn(100, dim).astype(np.float32)\n",
    "    data = [\n",
    "        list(range(100)),\n",
    "        vectors1,  # ivf_flat\n",
    "        vectors2,   # ivf_pq\n",
    "        vectors3,   # hnsw\n",
    "        vectors4    # diskann\n",
    "    ]\n",
    "    collection.insert(data)\n",
    "\n",
    "    logging.info(f\"Inserted {len(vectors1)} vectors\")\n",
    "\n",
    "    # 5. Verify search for each index type\n",
    "    search_params = {\n",
    "        \"ivf_flat\": {\"nprobe\": 32},\n",
    "        \"ivf_pq\": {\"nprobe\": 64},\n",
    "        \"hnsw\": {\"ef\": 128},\n",
    "        \"diskann\": {\"search_list\": 100}\n",
    "    }\n",
    "\n",
    "    for field in [\"ivf_flat\", \"ivf_pq\", \"hnsw\", \"diskann\"]:\n",
    "        logging.info(f\"Testing {field.upper()} search...\")\n",
    "        results = collection.search(\n",
    "            data[1][:1] if field == \"ivf_flat\" else \n",
    "            data[2][:1] if field == \"ivf_pq\" else\n",
    "            data[3][:1] if field == \"hnsw\" else\n",
    "            data[4][:1],\n",
    "            field,\n",
    "            param={\"metric_type\": \"L2\", \"params\": search_params[field]},\n",
    "            limit=5,\n",
    "            output_fields=[\"id\"]\n",
    "        )\n",
    "        logging.info(f\"{field} Top 3 results:\")\n",
    "        for hit in results[0][:3]:\n",
    "            logging.info(f\"ID: {hit.id} | Distance: {hit.distance:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    utility.drop_collection(\"multi_index_test\")\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持分区管理功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=1000)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=\"vector\",\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"string_field\", \"INVERTED\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    client.load_collection(collection_name)\n",
    "\n",
    "    # Create partitions\n",
    "    client.create_partition(\n",
    "        collection_name=collection_name,\n",
    "        partition_name=\"partition_alpha\"\n",
    "    )\n",
    "    client.create_partition(\n",
    "        collection_name=collection_name,\n",
    "        partition_name=\"partition_beta\"\n",
    "    )\n",
    "    logging.info(\"Created two partitions: partition_alpha and partition_beta\")\n",
    "\n",
    "    # Insert data into specific partitions\n",
    "    vector = np.random.rand(dim).tolist()\n",
    "    alpha_data = [{\"id\": 3001, \"vector\": vector, \"string_field\": \"alpha data\", \"int_field\": 100}]\n",
    "    beta_data = [{\"id\": 3002, \"vector\": vector, \"string_field\": \"beta data\", \"int_field\": 200}]\n",
    "    \n",
    "    client.insert(collection_name, alpha_data, partition_name=\"partition_alpha\")\n",
    "    client.insert(collection_name, beta_data, partition_name=\"partition_beta\")\n",
    "    logging.info(\"Inserted test data into different partitions\")\n",
    "\n",
    "    # Partition query validation\n",
    "    logging.info(\"\\n=== Partition Query Test ===\")\n",
    "    alpha_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[vector],\n",
    "        partition_names=[\"partition_alpha\"],\n",
    "        output_fields=[\"string_field\"],\n",
    "        limit=5\n",
    "    )\n",
    "    logging.info(f\"Alpha partition documents: {alpha_results[0]}\")\n",
    "\n",
    "    # Cross-partition search\n",
    "    cross_partition_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[vector],\n",
    "        partition_names=[\"partition_alpha\", \"partition_beta\"],\n",
    "        output_fields=[\"string_field\"],\n",
    "        limit=5\n",
    "    )\n",
    "    logging.info(f\"Cross-partition matches: {cross_partition_results[0]}\")\n",
    "\n",
    "    # Release partition validation\n",
    "    logging.info(\"\\n=== Partition Release Test ===\")\n",
    "    client.release_partitions(\n",
    "        collection_name=collection_name,\n",
    "        partition_names=\"partition_alpha\"\n",
    "    )\n",
    "    \n",
    "    # Verify release by attempting query\n",
    "    try:\n",
    "        client.query(\n",
    "            collection_name=collection_name,\n",
    "            partition_names=[\"partition_alpha\"],\n",
    "            filter=\"int_field >= 0\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Expected error after release: {str(e)}\")\n",
    "\n",
    "    # Drop partition validation\n",
    "    logging.info(\"\\n=== Partition Drop Test ===\")\n",
    "    client.drop_partition(\n",
    "        collection_name=collection_name,\n",
    "        partition_name=\"partition_alpha\"\n",
    "    )\n",
    "    \n",
    "    # Verify partition list\n",
    "    remaining_partitions = client.list_partitions(collection_name)\n",
    "    logging.info(f\"Remaining partitions: {remaining_partitions}\")\n",
    "    \n",
    "    # Verify drop by attempting insert\n",
    "    try:\n",
    "        client.insert(\n",
    "            collection_name=collection_name,\n",
    "            data=[{\"id\": 9999, \"vector\": vector}],\n",
    "            partition_name=\"partition_beta\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Expected error after drop: {str(e)}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持多个字段作为联合主键。[暂不支持]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持往数据表实时写入或更新一定量向量数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=1000)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200})\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=\"vector\",\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"string_field\", \"INVERTED\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    client.load_collection(collection_name)\n",
    "\n",
    "    # Real-time write validation\n",
    "    logging.info(\"\\n=== Real-time Write Test ===\")\n",
    "    \n",
    "    # High-frequency insert test\n",
    "    start_time = time.time()\n",
    "    vector = np.random.randn(dim).astype(np.float32).tolist()\n",
    "    real_time_data = [\n",
    "        {\"id\": 4001, \"vector\": vector, \"string_field\": \"live_update\", \"int_field\": 999},\n",
    "        {\"id\": 4002, \"vector\": vector, \"string_field\": \"streaming_data\", \"int_field\": 888}\n",
    "    ]\n",
    "    client.insert(collection_name, real_time_data)\n",
    "    logging.info(f\"Real-time insert completed in {time.time()-start_time:.4f}s\")\n",
    "\n",
    "    # Verify immediate searchability\n",
    "    search_res = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[vector],\n",
    "        filter=\"int_field >= 888\",\n",
    "        output_fields=[\"id\"],\n",
    "        limit=5\n",
    "    )\n",
    "    logging.info(f\"Real-time docs found: {search_res[0]}\")\n",
    "\n",
    "    # In-place update test\n",
    "    start_time = time.time()\n",
    "    client.insert(collection_name, [{\"id\": 4001, \"vector\": vector, \"string_field\": \"live_update\", \"int_field\": 1000}])\n",
    "    logging.info(f\"Document updated in {time.time()-start_time:.4f}s\")\n",
    "\n",
    "    # Verify update persistence\n",
    "    query_res = client.query(\n",
    "        collection_name=collection_name,\n",
    "        filter=\"id == 4001\",\n",
    "        output_fields=[\"int_field\"]\n",
    "    )\n",
    "    logging.info(f\"Updated value: {query_res[0]['int_field']}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持diskann等磁盘索引方案，以承载更大的数据量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=1000)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector\", \"DISKANN\", \"L2\", {\n",
    "            \"index_build_ram_gb\": 4,   # RAM allocation during build\n",
    "            \"search_cache_size\": 2,    # GB allocated for search cache\n",
    "            \"pq_code_budget_gb\": 0.1   # Compression ratio\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=field_name,\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"string_field\", \"INVERTED\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\",\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    \n",
    "    logging.info(\"=== Index Descriptions ===\")\n",
    "    diskann_info = client.describe_index(\n",
    "        collection_name=collection_name,\n",
    "        index_name=\"vector_DISKANN_index\"\n",
    "    )\n",
    "    logging.info(f\"Index Config:{diskann_info}\")\n",
    "    \n",
    "    client.load_collection(collection_name)\n",
    "\n",
    "    # Real-time write validation\n",
    "    logging.info(\"\\n=== Real-time Write Test ===\")\n",
    "    \n",
    "    # High-frequency insert test\n",
    "    vector = np.random.randn(dim).astype(np.float32).tolist()\n",
    "    real_time_data = [\n",
    "        {\"id\": 4001, \"vector\": vector, \"string_field\": \"live_update\", \"int_field\": 999},\n",
    "        {\"id\": 4002, \"vector\": vector, \"string_field\": \"streaming_data\", \"int_field\": 888}\n",
    "    ]\n",
    "    client.insert(collection_name, real_time_data)\n",
    "    client.flush(collection_name)\n",
    "\n",
    "     # DiskANN specific validation\n",
    "    logging.info(\"\\n=== DiskANN Validation ===\")\n",
    "    diskann_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[vector],\n",
    "        limit=5,\n",
    "    )\n",
    "    logging.info(f\"DiskANN search success: {diskann_results[0]}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否支持在单个数据表中构建多个索引，以支持对比测试或者开发校验调优等功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=1000)\n",
    "    \n",
    "    # Add four vector fields with same dimension\n",
    "    vector_fields = [\n",
    "        (\"vector_flat\", \"FLAT\"),\n",
    "        (\"vector_ivf\", \"IVF_FLAT\"),\n",
    "        (\"vector_hnsw\", \"HNSW\"),\n",
    "        (\"vector_diskann\", \"DISKANN\")\n",
    "    ]\n",
    "    \n",
    "    for field_name, _ in vector_fields:\n",
    "        schema.add_field(\n",
    "            field_name=field_name,\n",
    "            datatype=DataType.FLOAT_VECTOR,\n",
    "            dim=dim\n",
    "        )\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "\n",
    "    # 3. Index management\n",
    "    # Vector indexes\n",
    "    vector_indexes = [\n",
    "        (\"vector_flat\", \"FLAT\", \"L2\", {}),\n",
    "        (\"vector_ivf\", \"IVF_FLAT\", \"L2\", {\"nlist\": 128}),\n",
    "        (\"vector_hnsw\", \"HNSW\", \"L2\", {\"M\": 16, \"efConstruction\": 200}),\n",
    "        (\"vector_diskann\", \"DISKANN\", \"L2\", {\n",
    "            \"index_build_ram_gb\": 4,\n",
    "            \"search_cache_size\": 2\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        index_params.add_index(\n",
    "            field_name=field_name,\n",
    "            index_type=idx_type,\n",
    "            metric_type=metric,\n",
    "            index_name=f\"vector_{idx_type}_index\",\n",
    "\n",
    "        )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"string_field\", \"INVERTED\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\",\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params,\n",
    "    )\n",
    "    \n",
    "    logging.info(\"=== Index Descriptions ===\")\n",
    "    for field_name, idx_type, metric, params in vector_indexes:\n",
    "        diskann_info = client.describe_index(\n",
    "            collection_name=collection_name,\n",
    "            index_name=f\"vector_{idx_type}_index\"\n",
    "        )\n",
    "        logging.info(f\"Index Config:{diskann_info}\")\n",
    "    \n",
    "    client.load_collection(collection_name)\n",
    "    \n",
    "    # Insert test data with all vector columns\n",
    "    test_data = [{\n",
    "        \"id\": i,\n",
    "        \"vector_flat\": vec,\n",
    "        \"vector_ivf\": vec,\n",
    "        \"vector_hnsw\": vec,\n",
    "        \"vector_diskann\": vec,\n",
    "        \"string_field\": f\"text_{i}\",\n",
    "        \"int_field\": i\n",
    "    } for i, vec in enumerate([np.random.rand(dim).tolist() for _ in range(5)])]\n",
    "    client.insert(collection_name, test_data)\n",
    "    client.flush(collection_name)\n",
    "    \n",
    "    logging.info(\"=== Cross Vector Column Search ===\")\n",
    "    for field_name, idx_type, _, _ in vector_indexes:\n",
    "        results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            data=[test_data[0][field_name]],\n",
    "            anns_field=field_name,\n",
    "            search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10} if idx_type == \"IVF_FLAT\" else {}},\n",
    "            limit=5,\n",
    "        )\n",
    "        logging.info(f\"{field_name} found {len(results[0])} results\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证向量数据库是否提供混合检索的能力并提供不同检索方式的权重调整以适应不同类型的检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pymilvus import MilvusClient, DataType, Function, FunctionType\n",
    "import numpy as np\n",
    "import time\n",
    "from pymilvus import AnnSearchRequest, RRFRanker, WeightedRanker\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 1. Initialize MilvusClient\n",
    "    logging.info(\"Initializing MilvusClient...\")\n",
    "    client = MilvusClient(uri=\"http://127.0.0.1:19530\")\n",
    "    logging.info(\"Client initialized successfully\")\n",
    "\n",
    "    # 2. Collection management\n",
    "    collection_name = \"consistency_test\"\n",
    "    dim = 768\n",
    "    \n",
    "    # Cleanup existing collection\n",
    "    if client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "    \n",
    "    # Create schema explicitly\n",
    "    schema = client.create_schema(\n",
    "        auto_id=False,\n",
    "        enable_dynamic_field=True\n",
    "    )\n",
    "    schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "    schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=1000, enable_analyzer=True)  # For sparse vector generation\n",
    "    schema.add_field(field_name=\"sparse_vec\", datatype=DataType.SPARSE_FLOAT_VECTOR)  # Sparse vector field\n",
    "    schema.add_field(field_name=\"dense_vec\", datatype=DataType.FLOAT_VECTOR, dim=dim)  # Rename dense vector\n",
    "    schema.add_field(field_name=\"int_field\", datatype=DataType.INT32)\n",
    "    schema.add_field(field_name=\"string_field\", datatype=DataType.VARCHAR, max_length=1000)    \n",
    "    bm25_function = Function(\n",
    "        name=\"text_bm25_emb\", # Function name\n",
    "        input_field_names=[\"text\"], # Name of the VARCHAR field containing raw text data\n",
    "        output_field_names=[\"sparse_vec\"], # Name of the SPARSE_FLOAT_VECTOR field reserved to store generated embeddings\n",
    "        function_type=FunctionType.BM25,\n",
    "    )\n",
    "    schema.add_function(bm25_function)\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        schema=schema,\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "\n",
    "    index_params = client.prepare_index_params()\n",
    "    # Sparse vector index (BM25)\n",
    "    index_params.add_index(\n",
    "        field_name=\"sparse_vec\",\n",
    "        index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "        index_name=\"sparse_vec_index\",\n",
    "        metric_type=\"BM25\",\n",
    "        params={\"inverted_index_algo\": \"DAAT_MAXSCORE\"}\n",
    "    )\n",
    "    \n",
    "    # Dense vector index\n",
    "    index_params.add_index(\n",
    "        field_name=\"dense_vec\",\n",
    "        index_type=\"IVF_FLAT\",\n",
    "        metric_type=\"IP\",\n",
    "        index_name=\"dense_vec_index\",\n",
    "        params={\"nlist\": 128}\n",
    "    )\n",
    "        \n",
    "    # Scalar indexes\n",
    "    scalar_fields = [\n",
    "        (\"string_field\", \"INVERTED\"), \n",
    "        (\"int_field\", \"STL_SORT\")\n",
    "    ]\n",
    "\n",
    "    for field, idx_type in scalar_fields:\n",
    "        index_params.add_index(\n",
    "            field_name=field,\n",
    "            index_type=idx_type,\n",
    "            index_name=f\"{field}_index\",\n",
    "        )\n",
    "        \n",
    "    client.create_index(\n",
    "        collection_name=collection_name,\n",
    "        index_params=index_params,\n",
    "    )\n",
    "    \n",
    "    logging.info(\"=== Index Descriptions ===\")\n",
    "    for index_name in [\"sparse_vec_index\", \"dense_vec_index\"]:\n",
    "        diskann_info = client.describe_index(\n",
    "            collection_name=collection_name,\n",
    "            index_name=index_name\n",
    "        )\n",
    "        logging.info(f\"Index Config:{diskann_info}\")\n",
    "    \n",
    "    client.load_collection(collection_name)\n",
    "\n",
    "\n",
    "    docs = [\n",
    "        \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "        \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "        \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "        \"Turing's work laid the foundation for theoretical computer science.\",\n",
    "        \"He is also known for his contributions to cryptanalysis and computer security.\",\n",
    "    ]\n",
    "    # Insert test data with both vector types\n",
    "    test_data = [{\n",
    "        \"id\": i,\n",
    "        \"text\": docs[i],\n",
    "        \"dense_vec\": np.random.randn(dim).tolist(),\n",
    "        \"string_field\": f\"string_{i}\",\n",
    "        \"int_field\": i\n",
    "    } for i in range(5)]\n",
    "    client.insert(collection_name, test_data)\n",
    "\n",
    "    \n",
    "    logging.info(\"\\n=== Sparse+Dense Hybrid Search ===\")\n",
    "    # Create search requests\n",
    "    dense_request = AnnSearchRequest(\n",
    "        data=[test_data[0][\"dense_vec\"]],\n",
    "        anns_field=\"dense_vec\",\n",
    "        param={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}},\n",
    "        limit=5\n",
    "    )\n",
    "    \n",
    "    sparse_request = AnnSearchRequest(\n",
    "        data=[test_data[0][\"text\"]],  # Use text for BM25 sparse search\n",
    "        anns_field=\"sparse_vec\",\n",
    "        param={\"metric_type\": \"BM25\"},\n",
    "        limit=5\n",
    "    )\n",
    "    \n",
    "    # ranker = WeightedRanker(0.8, 0.3) \n",
    "    # ranker = WeightedRanker(0.3, 0.8) \n",
    "    ranker = RRFRanker(100)\n",
    "\n",
    "    # Execute hybrid search with RRF rerank\n",
    "    hybrid_results = client.hybrid_search(\n",
    "        collection_name=collection_name,\n",
    "        reqs=[dense_request, sparse_request],\n",
    "        ranker=RRFRanker(k=60),  # Reciprocal Rank Fusion\n",
    "        limit=5\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Final Reranked Results:\")\n",
    "    for idx, hit in enumerate(hybrid_results[0]):\n",
    "        logging.info(f\"Rank {idx+1}: ID={hit['id']} Score={hit['distance']:.3f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    logging.info(\"Cleaning up...\")\n",
    "    client.drop_collection(collection_name)\n",
    "    logging.info(\"Collection dropped successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
